import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
import lightgbm as lgb
from sklearn.model_selection import GridSearchCV
from scipy.spatial import ConvexHull, convex_hull_plot_2d
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

X_train = pd.read_csv('data/training_data/X_train.csv')
X_train.drop('id', axis = 1, inplace=True)
y_train = pd.read_csv('data/training_data/y_train.csv')
y_train.drop('id', axis = 1, inplace=True)
X_test = pd.read_csv('data/test_data/X_test.csv')
X_test.drop('id', axis = 1, inplace=True)
y_test = pd.read_csv('data/test_data/y_test.csv')
y_test.drop('id', axis = 1, inplace=True)

#fill na training data
X_train = X_train.apply(lambda x: x.fillna(x.mean()),axis=0)
X_test = X_test.apply(lambda x: x.fillna(x.mean()),axis=0)

#drop variables that worsen prediction
X_train.drop(['s_storage_size', 's_duration'], axis = 1, inplace=True)
X_test.drop(['s_storage_size', 's_duration'], axis = 1, inplace=True)


estimator = lgb.LGBMRegressor()

# this is the way I tested which parametes might be the best ones
param_grid = {
    'max_depth': [20],
    'num_leaves': [20],
    'learning_rate': [0.1],
    'n_estimators': [9000]
}

gbm = GridSearchCV(estimator, param_grid, cv=3)
gbm.fit(X_train, y_train)

print('Best parameters found by grid search are:', gbm.best_params_)
y_pred = gbm.predict(X_test)


# have a look at the results:
print('Mean Absolute Error: %.4f' %
      metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error: %.4f' % metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error: %.4f' %
      np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print('R2 score:%.4f' % r2_score(y_test, y_pred))
